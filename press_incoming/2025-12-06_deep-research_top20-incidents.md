Locate and compile reputable sources that document how untruthful output from Large Language Models (LLMs) or Vision-Language Models (VLMs) have caused, or could cause, significant economic or compliance-related harm—especially within finance or life sciences. Emphasize instances where such failures have already produced measurable monetary losses, highlighting the regulated nature of these sectors. Finally, present a comprehensive top-20 table with columns: URL | concise case synopsis | estimated monetary impact (USD) | additional commentary.

⸻

Top-20 cases and analyses

#	URL	Concise case synopsis	Estimated monetary impact (USD)	Additional commentary
1	https://www.businessinsider.com/airline-ordered-to-compensate-passenger-misled-by-chatbot-2024-2	Air Canada’s generative-AI chatbot fabricated a retroactive bereavement refund policy; the passenger relied on it, was denied the refund, and won before the British Columbia Civil Resolution Tribunal. ￼	≈ 700 USD (C$650.88 fare difference + C$36.14 interest + C$125 fees ≈ US$721). ￼	Regulated sector (air transport, consumer protection). Tribunal held the airline liable for “negligent misrepresentation” by its chatbot and rejected the argument that the bot was a “separate legal entity.” This is a clean, well-documented AI-hallucination → legal liability → direct cash outflow chain.
2	https://www.techradar.com/pro/deloitte-forced-to-refund-aussie-government-after-admitting-it-used-ai-to-produce-error-strewn-report	Deloitte used GPT-4o to draft a report for Australia’s Department of Employment and Workplace Relations; the report contained fake citations, non-existent quotations and other hallucinations, forcing the government to correct and re-upload it and Deloitte to repay the final installment of a AU$440k contract. ￼	Exact refund unknown; upper bound ≈ 290,000 USD (total contract AU$440k). Real refund was the “final instalment,” almost certainly a low six-figure amount.	Highly salient example of an LLM-generated expert-style document in a regulated public-policy context. The consulting firm had to refund the state and suffered reputational damage; the ministry had to spend internal resources to repair the report. Shows how hallucinated legal/administrative content can directly affect procurement and public-sector governance.
3	https://www.goldbergsegalla.com/news-and-knowledge/insights/fake-cases-real-consequences-two-recent-opinions-demonstrate-the-continuing-risks-of-generative-ai-in-litigation	Mata v. Avianca (S.D.N.Y.): lawyers submitted a brief with multiple non-existent cases generated by ChatGPT; the court sanctioned them US$5,000 and required notification of all affected judges. ￼	≥ 5,000 USD in direct sanctions, plus unquantified extra defense costs, internal reviews, and reputational damage.	Legal domain but strongly relevant to finance/life-science compliance: when legal teams in regulated industries lean on LLM research, hallucinated citations can directly trigger sanctions and weaken the client’s position.
4	https://www.washingtonpost.com/nation/2025/06/03/attorneys-court-ai-hallucinations-judges	Washington Post analysis using Damien Charlotin’s AI Hallucination Cases Database: at least 95 U.S. court matters since mid-2023 involve AI-hallucinated citations; in several, courts imposed fines of “thousands of dollars” on lawyers and firms. ￼	At least tens of thousands of USD in aggregate sanctions; exact total not yet systematically tallied.	Charlotin’s database is now a de facto registry of AI-tainted filings. In heavily regulated sectors (banking, pharma, med-tech) where litigation risk is structurally high, any legal team using LLMs without controls is now demonstrably exposed to monetary sanctions and malpractice / D&O risk.
5	https://www.paulhastings.com/insights/client-alerts/dismissal-of-false-claims-act-lawsuit-tainted-by-experts-ai-hallucinations-presents-cautionary-tale	U.S. ex rel. Khoury v. Intermountain Healthcare (D. Utah): a False Claims Act qui tam case about Medicare/Medicaid billing was dismissed after an expert report was found to contain generative-AI hallucinations (fake testimony, fictitious manual quotes). ￼	Not public; FCA cases of this type typically allege damages in the multi-million USD range, so the economic stakes for both relator and provider are potentially very large, even though no judgment was entered.	Healthcare + federal reimbursement = highly regulated. Here an AI-tainted expert report likely influenced DOJ’s decision to intervene and dismiss. This is a direct compliance failure triggered by hallucinated content in a life-sciences/healthcare context.
6	https://www.investopedia.com/nearly-1-in-5-people-who-took-financial-advice-from-ai-lost-at-least-100-dollars-doing-so-survey-finds-11806381	Pearl.com survey: nearly 1 in 5 Americans who followed AI-generated financial advice (ChatGPT and similar tools) lost more than US$100; losses were especially frequent among Gen Z investors. ￼	Per individual, ≥ 100 USD lost; at U.S. scale this implies at least tens of millions of USD in aggregate losses if survey rates generalize, though the article doesn’t publish a national total.	Clear evidence of realized economic harm from untruthful or context-insensitive LLM advice in personal finance (investments, crypto, etc.). These losses sit entirely outside regulated advice frameworks, raising questions for consumer-protection regulators (SEC, CFPB, FCA equivalents).
7	https://www.theguardian.com/technology/2025/nov/18/warning-ai-chatbots-inaccurate-financial-advice-tips-chatgpt-copilot-uk	Which? (UK) study of AI money chatbots (ChatGPT, Copilot, etc.) found inaccurate advice on tax, investments, and travel insurance, including prompts that could cause consumers to overpay tax or buy unnecessary cover. ￼	Monetary impact not quantified; but individual tax/insurance errors can easily reach hundreds or thousands of USD per consumer.	UK financial services are tightly regulated; giving misleading tax or insurance information can trigger remediation obligations and regulatory scrutiny. This is still “pre-incident” research, but in practice, consumers are already acting on such advice (see row 6).
8	https://www.unisg.ch/en/newsroom/be-careful-financial-advice-from-ai-comes-with-risks/	University of St. Gallen analysis tested several popular LLMs on portfolio-advice prompts and documented biased, short-termist, and sometimes factually wrong recommendations (e.g., overconcentration, ignoring risk profile). ￼	Not monetized in the paper; however, even a 5–10% underperformance on a US$10k–100k portfolio over a few years is a four- to five-figure loss.	Academic evidence that general-purpose LLMs encode behavioral and market biases. If banks or robo-advisors adopt such models naïvely, this becomes a systemic suitability and conduct-risk problem under MiFID II, FINSA, etc.
9	https://www.consumerfinance.gov/data-research/research-reports/chatbots-in-consumer-finance/chatbots-in-consumer-finance/	CFPB “Chatbots in Consumer Finance” issue spotlight: documents consumer complaints where bank chatbots mis-handled disputes, gave incorrect payoff/fee information, or trapped consumers in “doom loops,” potentially leading to illegal fees or missed deadlines. ￼	CFPB does not publish a numeric loss figure, but examples include wrongful collections and junk fees; aggregate exposure across U.S. banks is plausibly in the millions once restitution and penalties are accounted for.	While not all bots are LLMs, the report explicitly warns about generative, LLM-driven chatbots. For regulated banks, hallucinated or incomplete answers can trigger direct violations of consumer-finance law, supervisory findings, and enforcement actions.
10	https://www.swissbanking.ch/_Resources/Persistent/2/0/3/b/203b937e175f25819ea271c883a095fe1dfa1ee0/SBA_Generative-AI-in-Banking_EN.pdf	Swiss Bankers Association whitepaper on generative AI in banking: explains how LLM hallucinations (e.g., “yes” instead of “no”) can lead to persuasive but fundamentally wrong answers, and links this to data-protection, banking-secrecy and unfair-competition law under Swiss and EU regulation. ￼	Not tied to a single incident; describes risk channels where mis-advice or data misuse could lead to fines, remediation programs, or even criminal sanctions under Swiss Banking Act art. 47.	This is an authoritative sector-level mapping of hallucinations → legal exposure in a heavily regulated jurisdiction. It explicitly recommends RAG, guardrails, and human-in-the-loop for anything touching core banking decisions.
11	https://www.sec.gov/newsroom/speeches-statements/gurbir-remarks-pcce-041524	SEC Enforcement Director Gurbir Grewal’s 2024 speech: warns that AI-related disclosures must not be “materially false or misleading” and explicitly flags risks from AI tools that can hallucinate or generate inaccurate information for securities filings. ￼	No specific dollar figure yet, but the SEC has already brought enforcement actions against firms for AI-related misrepresentations, with penalties in the millions. ￼	In capital markets, if an issuer uses LLMs to draft MD&A, risk factors or marketing materials and they hallucinate improvements or capabilities, that becomes securities fraud exposure. Even before any specific hallucination-based case is public, enforcement for “AI-washing” shows the scale of likely future penalties.
12	https://www.nature.com/articles/s43856-025-01021-3	Multi-model assurance analysis in Nature: shows that multiple clinical LLMs are highly vulnerable to adversarial hallucinations during clinical decision support, producing confident but false recommendations when fed slightly perturbed prompts. ￼	No real-world dollar figure; but mis-triage, wrong drug, or delayed care can easily cost tens of thousands of USD per affected patient (ICU stays, malpractice payouts).	This is a core “could cause” paper in life sciences: it demonstrates that even state-of-the-art models can be systematically pushed into wrong answers. In regulated healthcare environments (FDA, EMA, national HTA bodies), such behavior is a direct challenge for safety cases and post-market surveillance.
13	https://annals.org/aim/fullarticle/doi/10.7326/L24-0190 (case discussion referenced in Harvard IATL blog)	Bromism case: a patient relied on online advice including ChatGPT to substitute ordinary salt with bromide-containing salt; he developed bromism (bromide poisoning) requiring hospital care. Harvard’s IATL blog uses this and similar cases to illustrate liability from LLM-generated medical advice. ￼	Article does not publish cost; U.S. inpatient toxicology admissions commonly run into the high four to low five figures per case.	This is a clinically documented instance where LLM-style advice contributed to a genuine toxicological emergency. Beyond direct medical costs, it creates product-liability and malpractice questions if clinicians or vendors promoted the tools.
14	https://www.newyorker.com/magazine/2025/09/29/if-ai-can-diagnose-patients-what-are-doctors-for	New Yorker feature on AI in diagnosis: cites studies where GPT-4 answered open-ended medical questions incorrectly about two-thirds of the time and GPT-3.5 misdiagnosed >80% of complex pediatric cases; also reports a U.S. survey where about one-fifth of Americans followed AI medical advice that later proved wrong, and an Arizona poison-control center saw fewer calls but more severe poisonings, plausibly because users relied on AI instead of calling. ￼	No explicit total, but even a small fraction of mis-advised patients generating an ICU stay (US$10–50k each) produces very large aggregate costs; survey numbers suggest national-scale exposure.	This is a linked set of empirical and epidemiological signals: high error rates, real behavior change (people trusting AI), and observable shifts in case severity at a poison center. It strongly supports the argument that LLM hallucinations are already impacting real-world healthcare utilization and risk.
15	https://pmc.ncbi.nlm.nih.gov/articles/PMC11250047/	Study “Assessing GPT-4’s Performance in Delivering Medical Advice”: compares GPT-4 guidance to human experts on real medical queries; while often high-quality, LLM outputs contain clinically significant errors and omissions that could lead to wrong self-management decisions without expert oversight. ￼	Not quantified; the paper frames risk qualitatively but notes that errors occur despite apparently safe language, i.e., good UX masking bad medicine.	This is one of several peer-reviewed studies feeding into regulatory science for AI in medicine; it gives evidence that “well-worded” but wrong advice is common enough to create non-trivial aggregate economic and liability risk for payers and providers.
16	https://iagen.unam.mx/recursos/People%20Overtrust%20AI-Generated%20Medical%20Advice%20despite%20Low%20Accuracy.pdf	“People Overtrust AI-Generated Medical Advice Despite Low Accuracy”: experimental work showing participants tend to view AI health answers as being as valid as physicians’ responses, even when accuracy is low, leading to inappropriate self-care intentions. ￼	No direct dollar values; but over- or under-use of care based on wrong triage decisions has well-known cost implications at population scale.	This is not a “case” but a rigorous behavioral mechanism study: it shows why hallucinations in medical LLMs translate into real utilization and malpractice risk, especially once such tools are integrated into patient portals or payer apps.
17	https://www.medrxiv.org/content/10.1101/2025.02.28.25323115v1.full-text	“Medical Hallucination in Foundation Models and Their Impact on Healthcare”: survey paper defining “medical hallucination” for both LLMs and medical VLMs (radiology, pathology), detailing failure modes and mapping them to patient-safety and regulatory risks. ￼	No direct cost figures; the paper explicitly argues that unmitigated hallucinations could “substantially increase adverse events and liability exposure” in clinical deployments.	This is a cornerstone for VLM-specific risk in life sciences. It consolidates evidence that image-text medical foundation models hallucinate findings or measurements – a direct route to misdiagnosis, unnecessary procedures, and malpractice payouts.
18	https://arxiv.org/html/2411.18672v1	“Mitigating Measurement Hallucinations in Chest X-ray Report Generation”: documents how a radiology-specialized multimodal model (MAIRA-2) hallucinates measurements and anatomical placement details in chest X-ray reports, and proposes mitigation strategies. ￼	Not tied to specific patients, but any mis-reported cardiothoracic ratio, lesion size, or laterality error can change therapeutic decisions; wrong interventions often cost tens of thousands of USD and can trigger malpractice claims.	This is an explicit demonstration that VLMs in imaging can fabricate clinically salient numerical details while sounding authoritative, which is exactly the kind of failure that would cause regulatory pushback for automated reporting in radiology departments.
19	https://money.com/can-you-trust-ai-financial-advice/	Money.com benchmark: staff asked ChatGPT (o3) and Gemini 2.5 Pro 25 personal-finance questions and had human experts grade responses; the models frequently produced incomplete, outdated or misleading guidance on debt payoff, retirement, and tax-advantaged accounts. ￼	Not monetized, but experts note that following some answers verbatim could cost individual households thousands of USD in missed employer matches, sub-optimal loan refinancing, or tax mistakes.	Together with rows 6–8, this builds a consistent picture: LLMs look like qualified financial coaches, but their hallucinations and omissions are sufficient to produce significant long-term wealth erosion if left uncorrected.
20	https://biztechmagazine.com/article/2025/08/llm-hallucinations-what-are-implications-financial-institutions	BizTech Magazine analysis on “LLM Hallucinations: What Are the Implications for Financial Institutions?”: synthesizes how hallucinated outputs can lead to mis-selling, incorrect KYC/AML assessments, flawed credit decisions, and inaccurate regulatory reporting if not tightly constrained. ￼	No single figure, but the article frames risk in terms of capital charges, conduct fines, remediation programs, and class-action litigation – each commonly reaching into the tens or hundreds of millions in financial services.	This is a sector-level risk translation: it connects the general phenomenon of hallucinations to very specific balance-sheet and compliance exposures (Basel capital, model-risk management, AML fines, etc.), making it directly usable for CRO/CFO discussions.


⸻

How this ties back to regulated finance and life sciences

A few synthesized points, tuned to your context:

☻ Fundamental insights
	•	In finance, we already see a continuum from small but real consumer losses (Pearl.com / Investopedia survey) to clear regulatory concern (CFPB, SEC, SwissBanking, BizTech). The pattern is:
➳ retail users act on unverified LLM advice →
➳ regulated firms consider embedding LLMs in advisory, customer service, and reporting →
➳ supervisors issue guidance and begin enforcement where outputs or disclosures are misleading. ￼
	•	In life sciences and healthcare, the evidence is shifting from “toy errors” to documented clinical incidents and strong mechanistic studies:
➳ a concrete toxicology case (bromism) where ChatGPT-style advice fed into patient behavior ￼
➳ population-level signals (poison-control trends, surveys of Americans following wrong AI medical advice) ￼
➳ technical papers showing both LLMs and VLMs hallucinate in precisely the domains that drive diagnostic and treatment decisions. ￼

♞ Pro-tips (how to weaponize this for your narrative)
	•	For boards / regulators:
➳ Start with row 1 and row 2 as “clean” case studies: small claim (Air Canada) and big-ticket consulting refund (Deloitte) – both show that hallucinations invalidate contracts and shift liability onto the deploying organization, not the model vendor.
➳ Then link to FCA/SEC/CFPB and SwissBanking material to show that supervisors already see hallucinations as a compliance problem, not just a UX bug.
	•	For healthcare / pharma QA & regulatory:
➳ Use rows 12–18 to show that the scientific community has converged on “medical hallucination” as a recognized safety category, specifically for LLMs and medical VLMs.
➳ Combine the bromism case and poison-center signal with Nature / medRxiv work to argue that ignoring hallucinations is incompatible with current expectations for clinical risk management and post-market surveillance.

➳ Necessary next steps if you want to build on this
	•	Build your own short “casebook” distilled from the URLs above with: incident description, model type, control failures, economic/compliance consequence, and mitigations.
	•	Map each failure mode to your own AI-governance controls (RAG vs base-LLM, human-in-the-loop, provenance, evaluation of hallucination rate, etc.) and to specific regulatory articles (FINMA/FINSA, GDPR/FADP, MDR/IVDR, FDA, FCA/SEC).
	•	For finance and life sciences clients, you can now credibly argue: “There is already case law and supervisory scrutiny linking hallucinations to monetary loss and liability. The question is not whether this happens, but how you limit it and document that limitation.”
