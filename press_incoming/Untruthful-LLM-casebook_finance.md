# Untruthful LLM Casebook: Banking, Investment, Trading, Accounting & Corporate Finance

| # | URL(s) | YYYY-MM | Entities involved | Concise Case synopsis | Additional commentary | Damage/Risk Estimate (USD) |
|---|--------|---------|-------------------|----------------------|----------------------|---------------------------|
| Fin1 | https://www.investopedia.com/nearly-1-in-5-people-who-took-financial-advice-from-ai-lost-at-least-100-dollars-doing-so-survey-finds-11806381 | 2024-11 (estimate) | ChatGPT, Pearl.com, Gen Z investors, surveyed Americans | Pearl.com survey: nearly 1 in 5 Americans who followed AI-generated financial advice (ChatGPT and similar tools) lost more than US$100; losses were especially frequent among Gen Z investors. | Clear evidence of realized economic harm from untruthful or context-insensitive LLM advice in personal finance (investments, crypto, etc.). These losses sit entirely outside regulated advice frameworks, raising questions for consumer-protection regulators (SEC, CFPB, FCA equivalents). Per individual, ≥ 100 USD lost; at U.S. scale this implies at least tens of millions of USD in aggregate losses if survey rates generalize. | ≥100 per individual; tens of millions aggregate (estimate) |
| Fin2 | https://www.theguardian.com/technology/2025/nov/18/warning-ai-chatbots-inaccurate-financial-advice-tips-chatgpt-copilot-uk | 2024-11 (estimate) | ChatGPT, Copilot, Which? (UK), consumers | Which? (UK) study of AI money chatbots (ChatGPT, Copilot, etc.) found inaccurate advice on tax, investments, and travel insurance, including prompts that could cause consumers to overpay tax or buy unnecessary cover. | UK financial services are tightly regulated; giving misleading tax or insurance information can trigger remediation obligations and regulatory scrutiny. This is still "pre-incident" research, but in practice, consumers are already acting on such advice. Monetary impact not quantified; but individual tax/insurance errors can easily reach hundreds or thousands of USD per consumer. | Hundreds to thousands per consumer |
| Fin3 | https://www.unisg.ch/en/newsroom/be-careful-financial-advice-from-ai-comes-with-risks/ | 2024-09 (estimate) | University of St. Gallen, popular LLMs, investors | University of St. Gallen analysis tested several popular LLMs on portfolio-advice prompts and documented biased, short-termist, and sometimes factually wrong recommendations (e.g., overconcentration, ignoring risk profile). | Academic evidence that general-purpose LLMs encode behavioral and market biases. If banks or robo-advisors adopt such models naïvely, this becomes a systemic suitability and conduct-risk problem under MiFID II, FINSA, etc. Not monetized in the paper; however, even a 5–10% underperformance on a US$10k–100k portfolio over a few years is a four- to five-figure loss. | 10,000–100,000 per portfolio (multi-year underperformance) |
| Fin4 | https://www.consumerfinance.gov/data-research/research-reports/chatbots-in-consumer-finance/chatbots-in-consumer-finance/ | 2023-06 | CFPB, U.S. banks, consumer finance chatbots | CFPB "Chatbots in Consumer Finance" issue spotlight: documents consumer complaints where bank chatbots mis-handled disputes, gave incorrect payoff/fee information, or trapped consumers in "doom loops," potentially leading to illegal fees or missed deadlines. Published 2023-06-06. | While not all bots are LLMs, the report explicitly warns about generative, LLM-driven chatbots. For regulated banks, hallucinated or incomplete answers can trigger direct violations of consumer-finance law, supervisory findings, and enforcement actions. CFPB does not publish a numeric loss figure, but examples include wrongful collections and junk fees; aggregate exposure across U.S. banks is plausibly in the millions once restitution and penalties are accounted for. | Millions aggregate (junk fees, restitution, penalties estimate) |
| Fin5 | https://www.swissbanking.ch/_Resources/Persistent/2/0/3/b/203b937e175f25819ea271c883a095fe1dfa1ee0/SBA_Generative-AI-in-Banking_EN.pdf | 2024-10 (estimate) | Swiss Bankers Association, Swiss banks, EU/Swiss regulators | Swiss Bankers Association whitepaper on generative AI in banking: explains how LLM hallucinations (e.g., "yes" instead of "no") can lead to persuasive but fundamentally wrong answers, and links this to data-protection, banking-secrecy and unfair-competition law under Swiss and EU regulation. | This is an authoritative sector-level mapping of hallucinations → legal exposure in a heavily regulated jurisdiction. It explicitly recommends RAG, guardrails, and human-in-the-loop for anything touching core banking decisions. Not tied to a single incident; describes risk channels where mis-advice or data misuse could lead to fines, remediation programs, or even criminal sanctions under Swiss Banking Act art. 47. | Criminal sanctions or fines possible; amount not specified |
| Fin6 | https://www.sec.gov/newsroom/speeches-statements/gurbir-remarks-pcce-041524 | 2024-04 | SEC, Enforcement Director Gurbir Grewal, issuers, firms | SEC Enforcement Director Gurbir Grewal's 2024 speech: warns that AI-related disclosures must not be "materially false or misleading" and explicitly flags risks from AI tools that can hallucinate or generate inaccurate information for securities filings. Speech delivered 2024-04-15. | In capital markets, if an issuer uses LLMs to draft MD&A, risk factors or marketing materials and they hallucinate improvements or capabilities, that becomes securities fraud exposure. Even before any specific hallucination-based case is public, enforcement for "AI-washing" shows the scale of likely future penalties. No specific dollar figure yet, but the SEC has already brought enforcement actions against firms for AI-related misrepresentations, with penalties in the millions. | Millions (AI-washing enforcement precedent) |
| Fin7 | https://money.com/can-you-trust-ai-financial-advice/ | 2025-08 | ChatGPT (o3), Gemini 2.5 Pro, Money.com, human experts | Money.com benchmark: staff asked ChatGPT (o3) and Gemini 2.5 Pro 25 personal-finance questions and had human experts grade responses; the models frequently produced incomplete, outdated or misleading guidance on debt payoff, retirement, and tax-advantaged accounts. Published 2025-08-18. | Together with Fin1–Fin3, this builds a consistent picture: LLMs look like qualified financial coaches, but their hallucinations and omissions are sufficient to produce significant long-term wealth erosion if left uncorrected. Not monetized, but experts note that following some answers verbatim could cost individual households thousands of USD in missed employer matches, sub-optimal loan refinancing, or tax mistakes. | Thousands per household (missed matches, tax errors) |
| Fin8 | https://biztechmagazine.com/article/2025/08/llm-hallucinations-what-are-implications-financial-institutions | 2025-08 | BizTech Magazine, financial institutions, regulators | BizTech Magazine analysis on "LLM Hallucinations: What Are the Implications for Financial Institutions?": synthesizes how hallucinated outputs can lead to mis-selling, incorrect KYC/AML assessments, flawed credit decisions, and inaccurate regulatory reporting if not tightly constrained. Published 2025-08-28. | This is a sector-level risk translation: it connects the general phenomenon of hallucinations to very specific balance-sheet and compliance exposures (Basel capital, model-risk management, AML fines, etc.), making it directly usable for CRO/CFO discussions. No single figure, but the article frames risk in terms of capital charges, conduct fines, remediation programs, and class-action litigation – each commonly reaching into the tens or hundreds of millions in financial services. | Tens to hundreds of millions (conduct fines, class actions) |
| Fin9 | https://arxiv.org/abs/2502.15865 | 2025-02 | Financial LLM Agents, Institutional Investment Managers, Research Community | Research study documents financial LLM agents hallucinating facts in high-stakes investment decisions. Agents generate false information about market conditions, misallocate portfolio decisions based on untruthful analyses. Researchers tested 6 financial agents; all showed critical vulnerabilities including hallucinated facts, stale data usage, and adversarial prompt vulnerability. | All 6 tested financial agents showed systematic vulnerabilities. Hallucinated facts, used stale data, vulnerable to adversarial prompts. Accuracy metrics create false sense of reliability. Projected $40M+ annual losses from LLM-generated false market analysis at institutional scale. Demonstrates unfaithful retrieval and hallucination in high-stakes portfolio management. | 40,000,000+ (projected institutional losses annually) |
| Fin10 | https://www.linkedin.com/posts/angelorodriguez_ai-artificialintelligence-defamation-activity-7395146086316785664-NxmC | 2024-11 (estimate) | Wolf River Electric, Google Gemini AI, Minnesota Attorney General, Federal Court | Google Gemini AI Overview generated entirely false claim: Wolf River Electric settled lawsuit with Minnesota AG over deceptive sales practices. Lawsuit never existed. AI hallucinated detailed false legal settlement affecting customer decisions. Customers demonstrably believed false AI-generated claims leading to documented business losses. | Documented real-world business loss: $388k immediate cancellations + $25M lost sales in 2024. Google continued false claims even after notification. Seeking $110M total damages. Establishes precedent for business harm from AI defamation and hallucinated legal claims. Shows how false LLM outputs directly impact B2B customer trust and revenue. | 25,000,000+ (documented lost sales 2024; 110M total damages sought) |
