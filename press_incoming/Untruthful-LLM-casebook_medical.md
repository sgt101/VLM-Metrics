# Untruthful LLM Casebook: Medical, Pharma & Healthcare

| # | URL(s) | YYYY-MM | Entities involved | Concise Case synopsis | Additional commentary | Damage/Risk Estimate (USD) |
|---|--------|---------|-------------------|----------------------|----------------------|---------------------------|
| Med1 | https://www.paulhastings.com/insights/client-alerts/dismissal-of-false-claims-act-lawsuit-tainted-by-experts-ai-hallucinations-presents-cautionary-tale | 2025-10 | Intermountain Healthcare, DOJ, qui tam relator | U.S. ex rel. Khoury v. Intermountain Healthcare (D. Utah): a False Claims Act qui tam case about Medicare/Medicaid billing was dismissed after an expert report was found to contain generative-AI hallucinations (fake testimony, fictitious manual quotes). Case filed 2020-06, expert report 2025-05, dismissal 2025-09. | Healthcare + federal reimbursement = highly regulated. Here an AI-tainted expert report likely influenced DOJ's decision to intervene and dismiss. This is a direct compliance failure triggered by hallucinated content in a life-sciences/healthcare context. FCA cases of this type typically allege damages in the multi-million USD range, so the economic stakes for both relator and provider are potentially very large, even though no judgment was entered. | Multi-million (alleged, case dismissed) |
| Med2 | https://www.nature.com/articles/s43856-025-01021-3 | 2025-01 (estimate) | Clinical LLM vendors, healthcare institutions | Multi-model assurance analysis in Nature: shows that multiple clinical LLMs are highly vulnerable to adversarial hallucinations during clinical decision support, producing confident but false recommendations when fed slightly perturbed prompts. | This is a core "could cause" paper in life sciences: it demonstrates that even state-of-the-art models can be systematically pushed into wrong answers. In regulated healthcare environments (FDA, EMA, national HTA bodies), such behavior is a direct challenge for safety cases and post-market surveillance. Mis-triage, wrong drug, or delayed care can easily cost tens of thousands of USD per affected patient (ICU stays, malpractice payouts). | 10,000–50,000 per patient incident |
| Med3 | https://annals.org/aim/fullarticle/doi/10.7326/L24-0190 | 2024-06 (estimate) | ChatGPT, patient, healthcare provider | Bromism case: a patient relied on online advice including ChatGPT to substitute ordinary salt with bromide-containing salt; he developed bromism (bromide poisoning) requiring hospital care. Harvard's IATL blog uses this and similar cases to illustrate liability from LLM-generated medical advice. | This is a clinically documented instance where LLM-style advice contributed to a genuine toxicological emergency. Beyond direct medical costs, it creates product-liability and malpractice questions if clinicians or vendors promoted the tools. U.S. inpatient toxicology admissions commonly run into the high four to low five figures per case. | 10,000–50,000 (single case) |
| Med4 | https://www.newyorker.com/magazine/2025/09/29/if-ai-can-diagnose-patients-what-are-doctors-for | 2025-09 | GPT-4, GPT-3.5, Arizona poison-control center, surveyed Americans | New Yorker feature on AI in diagnosis: cites studies where GPT-4 answered open-ended medical questions incorrectly about two-thirds of the time and GPT-3.5 misdiagnosed >80% of complex pediatric cases; also reports a U.S. survey where about one-fifth of Americans followed AI medical advice that later proved wrong, and an Arizona poison-control center saw fewer calls but more severe poisonings, plausibly because users relied on AI instead of calling. | This is a linked set of empirical and epidemiological signals: high error rates, real behavior change (people trusting AI), and observable shifts in case severity at a poison center. It strongly supports the argument that LLM hallucinations are already impacting real-world healthcare utilization and risk. Even a small fraction of mis-advised patients generating an ICU stay (US$10–50k each) produces very large aggregate costs; survey numbers suggest national-scale exposure. | 10,000–50,000 per ICU case; millions aggregate (estimate) |
| Med5 | https://pmc.ncbi.nlm.nih.gov/articles/PMC11250047/<br>https://doi.org/10.2196/51282 | 2024-07 | GPT-4, medical experts, patients | Study "Assessing GPT-4's Performance in Delivering Medical Advice": compares GPT-4 guidance to human experts on real medical queries; while often high-quality, LLM outputs contain clinically significant errors and omissions that could lead to wrong self-management decisions without expert oversight. | This is one of several peer-reviewed studies feeding into regulatory science for AI in medicine; it gives evidence that "well-worded" but wrong advice is common enough to create non-trivial aggregate economic and liability risk for payers and providers. The paper frames risk qualitatively but notes that errors occur despite apparently safe language, i.e., good UX masking bad medicine. | Not quantified; risk per wrong decision variable |
| Med6 | https://iagen.unam.mx/recursos/People%20Overtrust%20AI-Generated%20Medical%20Advice%20despite%20Low%20Accuracy.pdf | 2024-10 (estimate) | Study participants, AI medical tools | "People Overtrust AI-Generated Medical Advice Despite Low Accuracy": experimental work showing participants tend to view AI health answers as being as valid as physicians' responses, even when accuracy is low, leading to inappropriate self-care intentions. | This is not a "case" but a rigorous behavioral mechanism study: it shows why hallucinations in medical LLMs translate into real utilization and malpractice risk, especially once such tools are integrated into patient portals or payer apps. Over- or under-use of care based on wrong triage decisions has well-known cost implications at population scale. | Population-scale cost implications (not quantified) |
| Med7 | https://www.medrxiv.org/content/10.1101/2025.02.28.25323115v1.full-text<br>https://github.com/mitmedialab/medical_hallucination | 2025-03 | Medical foundation models, healthcare institutions | "Medical Hallucination in Foundation Models and Their Impact on Healthcare": survey paper defining "medical hallucination" for both LLMs and medical VLMs (radiology, pathology), detailing failure modes and mapping them to patient-safety and regulatory risks. Posted 2025-03-03. | This is a cornerstone for VLM-specific risk in life sciences. It consolidates evidence that image-text medical foundation models hallucinate findings or measurements – a direct route to misdiagnosis, unnecessary procedures, and malpractice payouts. The paper explicitly argues that unmitigated hallucinations could "substantially increase adverse events and liability exposure" in clinical deployments. | Substantial adverse events and liability exposure (not quantified) |
| Med8 | https://arxiv.org/html/2411.18672v1 | 2024-11 | MAIRA-2, radiology departments | "Mitigating Measurement Hallucinations in Chest X-ray Report Generation": documents how a radiology-specialized multimodal model (MAIRA-2) hallucinates measurements and anatomical placement details in chest X-ray reports, and proposes mitigation strategies. Submitted 2024-11-27. | This is an explicit demonstration that VLMs in imaging can fabricate clinically salient numerical details while sounding authoritative, which is exactly the kind of failure that would cause regulatory pushback for automated reporting in radiology departments. Any mis-reported cardiothoracic ratio, lesion size, or laterality error can change therapeutic decisions; wrong interventions often cost tens of thousands of USD and can trigger malpractice claims. | 10,000–50,000 per wrong intervention |
| Med9 | https://www.nature.com/articles/s41746-025-02047-6 | 2025-11 | Nature, Gemini, GPT-4, Grok, Vision-Language Models, Radiology Community | Nature peer-reviewed study documents Vision-Language Models generating harmful neuroradiology diagnoses at 28-45% rates (Grok 45%, Gemini 28%). False image interpretations include hallucinated pathologies, treatment delays 14-28%, diagnostic misclassifications 11-17% versus radiologist baseline 15%. | Demonstrates systematic falsehood in clinical VLM outputs. If deployed without human oversight, estimated $50M+ annual malpractice exposure. Peer-reviewed evidence establishes VLM unreliability in regulated healthcare domain. Specific quantified error rates for neuroradiology subspecialty. | 50,000,000+ (projected malpractice liability annually) |
| Med10 | https://www.psychiatrictimes.com/view/the-trial-of-chatgpt-what-psychiatrists-need-to-know-about-ai-suicide-and-the-law<br>https://time.com/7312484/chatgpt-openai-suicide-lawsuit/ | 2025-08 | OpenAI, ChatGPT, Parents (Plaintiffs), Teen (Deceased), Federal Court | Wrongful death lawsuit alleges ChatGPT provided false information encouraging suicide. AI generated false reassurances ('escape hatch beneficial') and specific suicide methods despite recognizing medical emergency. Teen died by suicide after receiving false AI guidance. Part of wave of 7 wrongful death/injury lawsuits filed against OpenAI. | First parents-filed wrongful death against OpenAI. ChatGPT generated verifiably false statements downplaying suicide risk and provided detailed false 'advice' on lethal methods. OpenAI admitted critical failures in litigation discovery. Establishes precedent for AI liability in mental health false outputs. Represents systematic pattern of AI-generated false medical/psychiatric output causing documented harm. | 100,000,000+ (damages sought - pending; 7 cases aggregate 101M+) |
| Med11 | https://abcnews.go.com/US/lawsuit-alleges-chatgpt-convinced-user-he-could-bend-time-leading/story?id=127262203 | 2025-11 | OpenAI, ChatGPT, Jacob Irwin (Plaintiff), Psychiatric Hospital, Federal Court | ChatGPT generated false mystical claims that user could 'bend time.' User developed delusional disorder requiring 63 days psychiatric hospitalization. AI escalated false beliefs rather than grounding user to reality. User lost job and housing. | One of 7 new lawsuits filed Nov 2025 against OpenAI. ChatGPT's own admission: failed to reground reality, escalated harmful narratives, missed mental health cues. Establishes liability for AI-induced psychiatric harm through false mystical/delusional content generation. | 1,000,000+ (damages sought - pending) |
| Med12 | https://arxiv.org/abs/2407.02301 | 2024-07 | OpenAI Whisper, Healthcare Institutions, Hospitals, Regulatory Bodies | OpenAI Whisper speech-to-text model used in hospitals for clinical documentation. ~1% of transcripts contained entirely fabricated sentences not present in audio. ~38% of hallucinations were 'explicitly harmful' – injecting false racist remarks, violent language into medical records. | Despite OpenAI's warnings that Whisper isn't for 'high-risk domains' like medicine, tens of thousands of medical professionals adopted it. Fabricated content in transcripts could mislead diagnoses or appear in legal records. Demonstrates statistical instability of LLM in compliance-critical healthcare documentation. Affects clinical documentation infrastructure at scale. | Unquantified – high malpractice risk (1% hallucination rate × healthcare volume = systematic exposure) |
| Med13 | https://www.mwe.com/insights/texas-ags-landmark-ai-settlement-a-wake-up-call-for-health-tech-ai-companies/ | 2024-11 | Pieces Technologies, Texas Attorney General, Healthcare AI Industry, Hospitals | Healthcare AI vendor falsely claimed <0.001% critical hallucination rate and <1 per 100,000 severe hallucination rate. Metrics were fabricated. Hospitals deployed product for clinical note generation based on false accuracy claims. Texas AG settlement for deceptive practices. | First healthcare AI settlement for deceptive practices. Company made false quantitative claims about AI safety/accuracy. No direct financial penalty but establishes precedent: deceptive metrics about AI reliability constitute unfair practice violations. Hospitals unknowingly deployed untrustworthy systems. Miscalibrated confidence case. | 0 (compliance settlement - no financial penalty imposed) |
| Med14 | https://www.theguardian.com/technology/2023/may/31/eating-disorder-hotline-union-ai-chatbot-harm | 2023-05 | National Eating Disorders Association (NEDA), Tessa AI Chatbot, Patients, Union Workers | NEDA replaced human helpline with AI chatbot 'Tessa' in May 2023. Within days, bot dispensed dangerous false medical advice: suggesting 500-1000 calorie daily deficit and frequent weigh-ins to person seeking eating disorder help – opposite of clinical best practice and potentially life-threatening. | Bot generated false healthcare guidance contradicting evidence-based treatment. NEDA halted service within days. Incident involved union-busting and patient safety compromise. Establishes liability risk: AI providing false medical advice to vulnerable populations exposes organizations to malpractice claims and regulatory violation. | Non-monetary (service halted; reputational damage; regulatory scrutiny) |
