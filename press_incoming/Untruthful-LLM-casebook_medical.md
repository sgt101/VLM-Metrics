# Untruthful LLM Casebook: Medical, Pharma & Healthcare

| # | URL(s) | YYYY-MM | Entities involved | Concise Case synopsis | Additional commentary |
|---|--------|---------|-------------------|----------------------|----------------------|
| Med1 | https://www.paulhastings.com/insights/client-alerts/dismissal-of-false-claims-act-lawsuit-tainted-by-experts-ai-hallucinations-presents-cautionary-tale | 2024-08 (estimate) | Intermountain Healthcare, DOJ, qui tam relator | U.S. ex rel. Khoury v. Intermountain Healthcare (D. Utah): a False Claims Act qui tam case about Medicare/Medicaid billing was dismissed after an expert report was found to contain generative-AI hallucinations (fake testimony, fictitious manual quotes). | Healthcare + federal reimbursement = highly regulated. Here an AI-tainted expert report likely influenced DOJ's decision to intervene and dismiss. This is a direct compliance failure triggered by hallucinated content in a life-sciences/healthcare context. FCA cases of this type typically allege damages in the multi-million USD range, so the economic stakes for both relator and provider are potentially very large, even though no judgment was entered. |
| Med2 | https://www.nature.com/articles/s43856-025-01021-3 | 2025-01 (estimate) | Clinical LLM vendors, healthcare institutions | Multi-model assurance analysis in Nature: shows that multiple clinical LLMs are highly vulnerable to adversarial hallucinations during clinical decision support, producing confident but false recommendations when fed slightly perturbed prompts. | This is a core "could cause" paper in life sciences: it demonstrates that even state-of-the-art models can be systematically pushed into wrong answers. In regulated healthcare environments (FDA, EMA, national HTA bodies), such behavior is a direct challenge for safety cases and post-market surveillance. Mis-triage, wrong drug, or delayed care can easily cost tens of thousands of USD per affected patient (ICU stays, malpractice payouts). |
| Med3 | https://annals.org/aim/fullarticle/doi/10.7326/L24-0190 | 2024-06 (estimate) | ChatGPT, patient, healthcare provider | Bromism case: a patient relied on online advice including ChatGPT to substitute ordinary salt with bromide-containing salt; he developed bromism (bromide poisoning) requiring hospital care. Harvard's IATL blog uses this and similar cases to illustrate liability from LLM-generated medical advice. | This is a clinically documented instance where LLM-style advice contributed to a genuine toxicological emergency. Beyond direct medical costs, it creates product-liability and malpractice questions if clinicians or vendors promoted the tools. U.S. inpatient toxicology admissions commonly run into the high four to low five figures per case. |
| Med4 | https://www.newyorker.com/magazine/2025/09/29/if-ai-can-diagnose-patients-what-are-doctors-for | 2025-09 | GPT-4, GPT-3.5, Arizona poison-control center, surveyed Americans | New Yorker feature on AI in diagnosis: cites studies where GPT-4 answered open-ended medical questions incorrectly about two-thirds of the time and GPT-3.5 misdiagnosed >80% of complex pediatric cases; also reports a U.S. survey where about one-fifth of Americans followed AI medical advice that later proved wrong, and an Arizona poison-control center saw fewer calls but more severe poisonings, plausibly because users relied on AI instead of calling. | This is a linked set of empirical and epidemiological signals: high error rates, real behavior change (people trusting AI), and observable shifts in case severity at a poison center. It strongly supports the argument that LLM hallucinations are already impacting real-world healthcare utilization and risk. Even a small fraction of mis-advised patients generating an ICU stay (US$10–50k each) produces very large aggregate costs; survey numbers suggest national-scale exposure. |
| Med5 | https://pmc.ncbi.nlm.nih.gov/articles/PMC11250047/ | 2024-07 (estimate) | GPT-4, medical experts, patients | Study "Assessing GPT-4's Performance in Delivering Medical Advice": compares GPT-4 guidance to human experts on real medical queries; while often high-quality, LLM outputs contain clinically significant errors and omissions that could lead to wrong self-management decisions without expert oversight. | This is one of several peer-reviewed studies feeding into regulatory science for AI in medicine; it gives evidence that "well-worded" but wrong advice is common enough to create non-trivial aggregate economic and liability risk for payers and providers. The paper frames risk qualitatively but notes that errors occur despite apparently safe language, i.e., good UX masking bad medicine. |
| Med6 | https://iagen.unam.mx/recursos/People%20Overtrust%20AI-Generated%20Medical%20Advice%20despite%20Low%20Accuracy.pdf | 2024-10 (estimate) | Study participants, AI medical tools | "People Overtrust AI-Generated Medical Advice Despite Low Accuracy": experimental work showing participants tend to view AI health answers as being as valid as physicians' responses, even when accuracy is low, leading to inappropriate self-care intentions. | This is not a "case" but a rigorous behavioral mechanism study: it shows why hallucinations in medical LLMs translate into real utilization and malpractice risk, especially once such tools are integrated into patient portals or payer apps. Over- or under-use of care based on wrong triage decisions has well-known cost implications at population scale. |
| Med7 | https://www.medrxiv.org/content/10.1101/2025.02.28.25323115v1.full-text | 2025-02 (estimate) | Medical foundation models, healthcare institutions | "Medical Hallucination in Foundation Models and Their Impact on Healthcare": survey paper defining "medical hallucination" for both LLMs and medical VLMs (radiology, pathology), detailing failure modes and mapping them to patient-safety and regulatory risks. | This is a cornerstone for VLM-specific risk in life sciences. It consolidates evidence that image-text medical foundation models hallucinate findings or measurements – a direct route to misdiagnosis, unnecessary procedures, and malpractice payouts. The paper explicitly argues that unmitigated hallucinations could "substantially increase adverse events and liability exposure" in clinical deployments. |
| Med8 | https://arxiv.org/html/2411.18672v1 | 2024-11 (estimate) | MAIRA-2, radiology departments | "Mitigating Measurement Hallucinations in Chest X-ray Report Generation": documents how a radiology-specialized multimodal model (MAIRA-2) hallucinates measurements and anatomical placement details in chest X-ray reports, and proposes mitigation strategies. | This is an explicit demonstration that VLMs in imaging can fabricate clinically salient numerical details while sounding authoritative, which is exactly the kind of failure that would cause regulatory pushback for automated reporting in radiology departments. Any mis-reported cardiothoracic ratio, lesion size, or laterality error can change therapeutic decisions; wrong interventions often cost tens of thousands of USD and can trigger malpractice claims. |
