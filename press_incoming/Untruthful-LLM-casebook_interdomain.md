# Untruthful LLM Casebook: Interdomain & Cross-Sector Incidents

| # | URL(s) | YYYY-MM | Entities involved | Concise Case synopsis | Additional commentary | Damage/Risk Estimate (USD) |
|---|--------|---------|-------------------|----------------------|----------------------|---------------------------|
| Int1 | <https://www.businessinsider.com/airline-ordered-to-compensate-passenger-misled-by-chatbot-2024-2> | 2024-02 | Air Canada, passenger, British Columbia Civil Resolution Tribunal | Air Canada's generative-AI chatbot **fabricated a retroactive bereavement refund policy**; the passenger relied on it, was denied the refund, and won before the British Columbia Civil Resolution Tribunal. | Regulated sector (air transport, consumer protection). Tribunal held the airline liable for "negligent misrepresentation" by its chatbot and rejected the argument that the bot was a "separate legal entity." This is a clean, well-documented AI-hallucination → legal liability → direct cash outflow chain. | ~ 1000 USD reimbursement |
| Int2 | <https://www.techradar.com/pro/deloitte-forced-to-refund-aussie-government-after-admitting-it-used-ai-to-produce-error-strewn-report> | 2025-10 | Deloitte, GPT-4o, Australia's Department of Employment and Workplace Relations | Deloitte used GPT-4o to draft a report for Australia's Department of Employment and Workplace Relations; the report contained **fake citations, non-existent quotations** and other hallucinations, forcing the government to correct and re-upload it and Deloitte to repay the final installment of a AU$440k contract. Article published 2025-10-07. | Highly salient example of an LLM-generated expert-style document in a regulated public-policy context. The consulting firm had to refund the state and suffered reputational damage; the ministry had to spend internal resources to repair the report. Shows how hallucinated legal/administrative content can directly affect procurement and public-sector governance. | Final installment refunded; upper bound 290,000 (AU$440k total contract) |
| Int3 | <https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/> | 2023-06  | ChatGPT, Mata v. Avianca lawyers, S.D.N.Y. court | Mata v. Avianca (S.D.N.Y.): lawyers submitted a brief with **multiple non-existent cases generated by ChatGPT**; the court sanctioned them US$5,000 and required notification of all affected judges. | Legal domain but strongly relevant to finance/life-science compliance: when legal teams in regulated industries lean on LLM research, hallucinated citations can directly trigger sanctions and weaken the client's position. | 5,000 direct sanctions + defense costs, reviews (not quantified) |
| Int4 | <https://www.washingtonpost.com/nation/2025/06/03/attorneys-court-ai-hallucinations-judges><br><https://arxiv.org/pdf/2405.20362.pdf> | 2025-06 | Damien Charlotin's AI Hallucination Cases Database, U.S. courts, lawyers/firms, Legal Research LLMs, ABA | Washington Post analysis using Damien Charlotin's AI Hallucination Cases Database: at least **95 U.S. court matters since mid-2023 involve AI-hallucinated citations**; in several, courts imposed fines of "thousands of dollars" on lawyers and firms. Legal research LLMs persistently **hallucinate false case law** despite tools claiming 'hallucination elimination' via RAG. ABA issued ethics opinion July 2024 on lawyer liability for AI hallucinations. | Charlotin's database is now a de facto registry of AI-tainted filings. In heavily regulated sectors (banking, pharma, med-tech) where litigation risk is structurally high, any legal team using LLMs without controls is now demonstrably exposed to monetary sanctions and malpractice / D&O risk. Legal tools (Casetext, Thomson Reuters) falsely claim hallucination elimination. Industry-wide pattern of false legal outputs with cumulative sanctions reaching $500K+ (2023-2025). | 500,000+ (cumulative sanctions 2023-2025 across 95+ documented cases) |
| Int5 | <https://www.nytimes.com/2025/07/08/us/judge-fines-lawyers-mypillow-ai.html><br><https://coloradosun.com/2025/07/07/mike-lindell-attorneys-fined-artificial-intelligence/> | 2025-07 | Mike Lindell attorneys (Christopher Kachouroff, Jennifer DeMaster), ChatGPT, Judge Nina Wang, U.S. District Court Denver | Two attorneys representing Mike Lindell in defamation lawsuit filed motion using generative AI. Document contained **~30 defective citations including non-existent cases, misquotations, fabricated legal precedent**. Judge Wang found intentional misconduct rather than accident based on conflicting statements about whether motion was draft. | Judge Nina Wang imposed $3,000 sanctions on each attorney for ChatGPT-generated falsehoods presented as real case law. Ruling emphasized this was "the least severe sanction adequate to deter and punish" their conduct. Clear example of LLM hallucinating false legal citations in high-stakes litigation. Part of growing pattern of legal hallucination sanctions. | 6,000 (combined sanctions: 3k per attorney) |
| Int6 | <https://ppc.land/activist-sues-google-over-ai-generated-false-claims-in-second-tech-lawsuit/> | 2025-10 | Robby Starbuck, Google Bard/Gemini, Delaware Superior Court | Google Bard/Gemini **falsely linked Starbuck to sexual assault allegations and white nationalist Richard Spencer**. AI **hallucinated criminal accusations** without factual basis. Lawsuit filed Oct 2025 in Delaware Superior Court. | Google fighting rather than settling (unlike Meta with same plaintiff). Demonstrates AI generating false defamatory statements about real persons. No court award for AI defamation damages yet established but precedent developing. Active litigation representing cutting edge of AI liability law. | 15,000,000 (damages sought - pending) |
| Int7 | <https://www.cnn.com/2025/08/14/tech/robby-starbuck-meta-ai-advisor> | 2025-08 | Robby Starbuck, Meta AI, Settlement Agreement | Meta AI **falsely stated Starbuck was involved in January 6 Capitol assault**. AI **hallucinated criminal association**. Settlement reached Aug 2025; amount undisclosed. Starbuck granted advisory role instead of transparent damages payment. | Meta chose settlement over litigation. AI generated false criminal accusations. First major settlement for AI hallucination-based defamation. Reflects tech industry risk avoidance on false AI outputs. Sets precedent for defamation liability and shows companies will settle to avoid discovery/trial. | Undisclosed (estimated 1–5M based on settlement structure and advisory role) |
| Int8 | <https://www.theguardian.com/technology/2025/sep/05/anthropic-settlement-ai-book-lawsuit> | 2023-10 (estimate) | Australian Mayor, ChatGPT, OpenAI, Legal Counsel | ChatGPT **falsely asserted Australian mayor had been convicted and imprisoned for bribery**. In reality, **he was the whistleblower** in that scandal with no charges. AI's false claims spread to constituents, prompting mayor's lawyers to send OpenAI legal warning. | Mayor prepared what could have been first defamation lawsuit against AI chatbot. Australian defamation awards capped around A$400K (~$270K). Mayor's counsel indicated seeking >A$200K given serious reputational harm. Spotlights AI's capacity to slander individuals. Case prepared but didn't proceed - still demonstrates international defamation risk. | 270,000 (potential damages estimate; case did not proceed to trial) |
| Int9 | <https://en.wikipedia.org/wiki/Mata_v._Avianca,_Inc.> | 2023-06 | Georgia Radio Host, ChatGPT, OpenAI, Federal Court | Georgia radio host **falsely accused by ChatGPT of fraud/embezzlement in fabricated court case**. Reporter's query to ChatGPT led bot to falsely describe host as defendant in **fictional embezzlement litigation**. Host sued OpenAI for defamation (Walters v. OpenAI, 2023). | Court ruled AI output wasn't made with 'actual malice' and noted ChatGPT's disclaimers about errors. Suit dismissed without damages. However, early bellwether for AI libel. Judge implied OpenAI's warnings and error-reduction efforts helped avoid liability. Establishes threshold for AI defamation claims requiring actual malice standard. | 0 (case dismissed without damages - but sets important legal precedent) |
| Int10 | <https://arxiv.org/abs/2502.15865> | 2025-02 | Colombian Judge, Cartagena Court, ChatGPT, Autistic Child, Healthcare System | Colombian judge disclosed using **ChatGPT to help decide case** about autistic child's medical coverage. Asked AI whether therapy costs legally exempt from fees; ChatGPT answered 'Yes,' aligning with judge's ruling. Decision happened to be correct but **highlighted AI instability**. | First-of-its-kind judicial disclosure of unvetted AI advice in case decision. Sparked controversy among judicial peers. Experts warned ChatGPT gives inconsistent answers and 'invents compelling lies' to same legal question. Highlighted instability of LLM outputs in justice system. International precedent for judicial AI use. | No direct cost (decision was correct - but illustrates systemic risk of judicial AI reliance) |
| Int11 | <https://www.theguardian.com/technology/2023/feb/09/google-ai-chatbot-bard-error-sends-shares-plummeting-in-battle-with-microsoft> | 2023-02 | Google, Bard (Gemini), James Webb Space Telescope, Investors, Alphabet | During Feb 2023 promotional demo, Google's Bard chatbot confidently gave **false answer: claiming James Webb Space Telescope took first exoplanet image (it didn't)**. Mistake immediately spotted by observers. Investors recoiled at 'AI flub,' fearing Google lagged competitors in AI race. | Bad press and perceived AI inaccuracy wiped $100B of Alphabet market capitalization in 24 hours. High-cost hallucination in public demo context. Underscored how unstable outputs erode trust and competitive positioning. Google forced to double-down on internal testing, risking ceding market ground to rivals. Demonstrates massive economic impact from single false factual claim. | 100,000,000,000 (market cap loss in single day) |
| Int12 | <https://futurism.com/internet-horrified-cnet-articles-written-ai> | 2023-01 (estimate) | CNET, Red Ventures (Parent), AI Journalist System, Financial Explainers, Editors | Tech site CNET deployed 'AI journalist' to write 77 financial explainer articles. Content found **riddled with factual mistakes and plagiarism**. Over half the AI-written articles required substantial corrections after watchdog report. CNET paused AI content; parent company laid off ~10% staff afterward. | Had to append editor's warnings to all AI-generated stories. Compliance risks in publishing (accuracy, copyright). Parent company claimed layoffs 'unrelated' to AI debacle but insiders reported morale and SEO standings suffered. Shows brand damage and job loss from unreliable LLM output in journalism/media sector. | Hard to quantify (cost of retractions, layoffs, reputational damage, SEO impact) |
| Int13 | <https://www.heise.de/en/news/AI-Fabricated-Source-Information-Burdens-Archives-and-Libraries-11107946.html><br><https://www.icrc.org/en/article/important-notice-ai-generated-archival-references> | 2025-12 | International Committee of the Red Cross (ICRC), Archives and Libraries worldwide, AI systems (unspecified), Librarians, Researchers | AI systems generating **fabricated archival references: false catalog numbers, nonexistent document descriptions, invented titles**, and false references to archival platforms. ICRC issued public statement addressing systematic problem. Researchers waste time attempting to verify nonexistent sources, creating "unnecessary extra work" for librarians and archival staff. | ICRC explicitly stated "AI systems do not research, do not verify sources, or check information." Creates loss of institutional trust in archives, significant confusion among researchers, and operational burden on library staff who must research false leads before discovering titles don't exist. Demonstrates AI hallucination affecting research infrastructure and scholarly work. Affects compliance in sectors requiring verified documentation (legal, healthcare, finance). | Not quantified (operational costs: staff time verifying false references, loss of institutional trust, research delays) |
