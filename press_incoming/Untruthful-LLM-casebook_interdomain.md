# Untruthful LLM Casebook: Interdomain & Cross-Sector Incidents

| # | URL(s) | YYYY-MM | Entities involved | Concise Case synopsis | Additional commentary | Damage/Risk Estimate (USD) |
|---|--------|---------|-------------------|----------------------|----------------------|---------------------------|
| Int1 | https://www.businessinsider.com/airline-ordered-to-compensate-passenger-misled-by-chatbot-2024-2 | 2024-02 | Air Canada, passenger, British Columbia Civil Resolution Tribunal | Air Canada's generative-AI chatbot fabricated a retroactive bereavement refund policy; the passenger relied on it, was denied the refund, and won before the British Columbia Civil Resolution Tribunal. | Regulated sector (air transport, consumer protection). Tribunal held the airline liable for "negligent misrepresentation" by its chatbot and rejected the argument that the bot was a "separate legal entity." This is a clean, well-documented AI-hallucination → legal liability → direct cash outflow chain. | 721 (C$812 total: C$650.88 fare + C$36.14 interest + C$125 fees) |
| Int2 | https://www.techradar.com/pro/deloitte-forced-to-refund-aussie-government-after-admitting-it-used-ai-to-produce-error-strewn-report | 2025-10 | Deloitte, GPT-4o, Australia's Department of Employment and Workplace Relations | Deloitte used GPT-4o to draft a report for Australia's Department of Employment and Workplace Relations; the report contained fake citations, non-existent quotations and other hallucinations, forcing the government to correct and re-upload it and Deloitte to repay the final installment of a AU$440k contract. Article published 2025-10-07. | Highly salient example of an LLM-generated expert-style document in a regulated public-policy context. The consulting firm had to refund the state and suffered reputational damage; the ministry had to spend internal resources to repair the report. Shows how hallucinated legal/administrative content can directly affect procurement and public-sector governance. | Final installment refunded; upper bound 290,000 (AU$440k total contract) |
| Int3 | https://www.goldbergsegalla.com/news-and-knowledge/insights/fake-cases-real-consequences-two-recent-opinions-demonstrate-the-continuing-risks-of-generative-ai-in-litigation | 2023-06 (estimate) | ChatGPT, Mata v. Avianca lawyers, S.D.N.Y. court | Mata v. Avianca (S.D.N.Y.): lawyers submitted a brief with multiple non-existent cases generated by ChatGPT; the court sanctioned them US$5,000 and required notification of all affected judges. | Legal domain but strongly relevant to finance/life-science compliance: when legal teams in regulated industries lean on LLM research, hallucinated citations can directly trigger sanctions and weaken the client's position. | 5,000 direct sanctions + defense costs, reviews (not quantified) |
| Int4 | https://www.washingtonpost.com/nation/2025/06/03/attorneys-court-ai-hallucinations-judges<br>https://arxiv.org/pdf/2405.20362.pdf | 2025-06 | Damien Charlotin's AI Hallucination Cases Database, U.S. courts, lawyers/firms, Legal Research LLMs, ABA | Washington Post analysis using Damien Charlotin's AI Hallucination Cases Database: at least 95 U.S. court matters since mid-2023 involve AI-hallucinated citations; in several, courts imposed fines of "thousands of dollars" on lawyers and firms. Legal research LLMs persistently hallucinate false case law despite tools claiming 'hallucination elimination' via RAG. ABA issued ethics opinion July 2024 on lawyer liability for AI hallucinations. | Charlotin's database is now a de facto registry of AI-tainted filings. In heavily regulated sectors (banking, pharma, med-tech) where litigation risk is structurally high, any legal team using LLMs without controls is now demonstrably exposed to monetary sanctions and malpractice / D&O risk. Legal tools (Casetext, Thomson Reuters) falsely claim hallucination elimination. Industry-wide pattern of false legal outputs with cumulative sanctions reaching $500K+ (2023-2025). | 500,000+ (cumulative sanctions 2023-2025 across 95+ documented cases) |
| Int5 | https://www.nytimes.com/2025/07/08/us/judge-fines-lawyers-mypillow-ai.html | 2025-07 | Mike Lindell's Attorneys, ChatGPT, Judge Nina Wang, Denver Federal Court | Two attorneys representing Mike Lindell filed defamation motion using generative AI. Document contained ~30 defective citations including non-existent cases, misquotations, fabricated legal precedent. Filed in Denver federal court July 2025. | Judge Nina Wang imposed sanctions for ChatGPT-generated falsehoods presented as real case law. Attorneys admitted using AI but submitted flawed brief anyway. Clear example of LLM hallucinating false legal citations in high-stakes litigation. Part of growing pattern of legal hallucination sanctions. | 6,000 (combined sanctions: 3k per attorney) |
| Int6 | https://coloradosun.com/2025/07/07/mike-lindell-attorneys-fined-artificial-intelligence/ | 2025-07 | California Appeals Attorney, ChatGPT, California Court of Appeal | California appeals attorney filed brief with 21 of 23 quoted precedents being fake, all produced by ChatGPT. Court found brief to be largely AI-generated fiction, exposing systematic failure to verify sources. | California's largest sanction to date for AI-fabricated legal filings. $10K fine. Blistering judicial opinion warned no court filing should contain citations lawyer hasn't personally verified. Prompted state-wide judicial guidance requiring courts adopt AI-use policies. Major state-level precedent. | 10,000 (California's largest AI-related sanction to date) |
| Int7 | https://ppc.land/activist-sues-google-over-ai-generated-false-claims-in-second-tech-lawsuit/ | 2025-10 | Robby Starbuck, Google Bard/Gemini, Delaware Superior Court | Google Bard/Gemini falsely linked Starbuck to sexual assault allegations and white nationalist Richard Spencer. AI hallucinated criminal accusations without factual basis. Lawsuit filed Oct 2025 in Delaware Superior Court. | Google fighting rather than settling (unlike Meta with same plaintiff). Demonstrates AI generating false defamatory statements about real persons. No court award for AI defamation damages yet established but precedent developing. Active litigation representing cutting edge of AI liability law. | 15,000,000 (damages sought - pending) |
| Int8 | https://www.cnn.com/2025/08/14/tech/robby-starbuck-meta-ai-advisor | 2025-08 | Robby Starbuck, Meta AI, Settlement Agreement | Meta AI falsely stated Starbuck was involved in January 6 Capitol assault. AI hallucinated criminal association. Settlement reached Aug 2025; amount undisclosed. Starbuck granted advisory role instead of transparent damages payment. | Meta chose settlement over litigation. AI generated false criminal accusations. First major settlement for AI hallucination-based defamation. Reflects tech industry risk avoidance on false AI outputs. Sets precedent for defamation liability and shows companies will settle to avoid discovery/trial. | Undisclosed (estimated 1–5M based on settlement structure and advisory role) |
| Int9 | https://www.theguardian.com/technology/2025/sep/05/anthropic-settlement-ai-book-lawsuit | 2023-10 (estimate) | Australian Mayor, ChatGPT, OpenAI, Legal Counsel | ChatGPT falsely asserted Australian mayor had been convicted and imprisoned for bribery. In reality, he was the whistleblower in that scandal with no charges. AI's false claims spread to constituents, prompting mayor's lawyers to send OpenAI legal warning. | Mayor prepared what could have been first defamation lawsuit against AI chatbot. Australian defamation awards capped around A$400K (~$270K). Mayor's counsel indicated seeking >A$200K given serious reputational harm. Spotlights AI's capacity to slander individuals. Case prepared but didn't proceed - still demonstrates international defamation risk. | 270,000 (potential damages estimate; case did not proceed to trial) |
| Int10 | https://incidentdatabase.ai/cite/40/ | 2016-ongoing | COMPAS Algorithm, Criminal Justice System, 1M+ Defendants, Courts | Algorithm used in 1M+ US defendant sentencings. Generated false risk classifications: Black defendants 45% more likely falsely classified high-risk; White defendants 23% false high-risk rate. Concordance accuracy only 61-63.6%. | Algorithm generates systematically untruthful risk predictions. Drives wrongful imprisonment settlements, appeals, systemic justice costs. False predictions disproportionately harm Black defendants. Demonstrates algorithmic bias producing false risk assessments at scale. Systemic justice domain with massive deployment footprint. | 20,000,000+ (systemic impact on wrongful imprisonment costs and appeals) |
| Int11 | https://en.wikipedia.org/wiki/Mata_v._Avianca,_Inc. | 2023-06 | Georgia Radio Host, ChatGPT, OpenAI, Federal Court | Georgia radio host falsely accused by ChatGPT of fraud/embezzlement in fabricated court case. Reporter's query to ChatGPT led bot to falsely describe host as defendant in fictional embezzlement litigation. Host sued OpenAI for defamation (Walters v. OpenAI, 2023). | Court ruled AI output wasn't made with 'actual malice' and noted ChatGPT's disclaimers about errors. Suit dismissed without damages. However, early bellwether for AI libel. Judge implied OpenAI's warnings and error-reduction efforts helped avoid liability. Establishes threshold for AI defamation claims requiring actual malice standard. | 0 (case dismissed without damages - but sets important legal precedent) |
| Int12 | https://arxiv.org/abs/2502.15865 | 2025-02 | Colombian Judge, Cartagena Court, ChatGPT, Autistic Child, Healthcare System | Colombian judge disclosed using ChatGPT to help decide case about autistic child's medical coverage. Asked AI whether therapy costs legally exempt from fees; ChatGPT answered 'Yes,' aligning with judge's ruling. Decision happened to be correct but highlighted AI instability. | First-of-its-kind judicial disclosure of unvetted AI advice in case decision. Sparked controversy among judicial peers. Experts warned ChatGPT gives inconsistent answers and 'invents compelling lies' to same legal question. Highlighted instability of LLM outputs in justice system. International precedent for judicial AI use. | No direct cost (decision was correct - but illustrates systemic risk of judicial AI reliance) |
| Int13 | https://www.theverge.com/news/757537/meta-robby-starbuck-conservative-activist-ai-bias-advisor | 2023-02 | Google, Bard (Gemini), James Webb Space Telescope, Investors, Alphabet | During Feb 2023 promotional demo, Google's Bard chatbot confidently gave false answer: claiming James Webb Space Telescope took first exoplanet image (it didn't). Mistake immediately spotted by observers. Investors recoiled at 'AI flub,' fearing Google lagged competitors in AI race. | Bad press and perceived AI inaccuracy wiped $100B of Alphabet market capitalization in 24 hours. High-cost hallucination in public demo context. Underscored how unstable outputs erode trust and competitive positioning. Google forced to double-down on internal testing, risking ceding market ground to rivals. Demonstrates massive economic impact from single false factual claim. | 100,000,000,000 (market cap loss in single day) |
| Int14 | https://futurism.com/internet-horrified-cnet-articles-written-ai | 2023-01 (estimate) | CNET, Red Ventures (Parent), AI Journalist System, Financial Explainers, Editors | Tech site CNET deployed 'AI journalist' to write 77 financial explainer articles. Content found riddled with factual mistakes and plagiarism. Over half the AI-written articles required substantial corrections after watchdog report. CNET paused AI content; parent company laid off ~10% staff afterward. | Had to append editor's warnings to all AI-generated stories. Compliance risks in publishing (accuracy, copyright). Parent company claimed layoffs 'unrelated' to AI debacle but insiders reported morale and SEO standings suffered. Shows brand damage and job loss from unreliable LLM output in journalism/media sector. | Hard to quantify (cost of retractions, layoffs, reputational damage, SEO impact) |
