# Untruthful LLM Casebook: Interdomain & Cross-Sector Incidents

| # | URL(s) | YYYY-MM | Entities involved | Concise Case synopsis | Additional commentary |
|---|--------|---------|-------------------|----------------------|----------------------|
| Int1 | https://www.businessinsider.com/airline-ordered-to-compensate-passenger-misled-by-chatbot-2024-2 | 2024-02 | Air Canada, passenger, British Columbia Civil Resolution Tribunal | Air Canada's generative-AI chatbot fabricated a retroactive bereavement refund policy; the passenger relied on it, was denied the refund, and won before the British Columbia Civil Resolution Tribunal. | Regulated sector (air transport, consumer protection). Tribunal held the airline liable for "negligent misrepresentation" by its chatbot and rejected the argument that the bot was a "separate legal entity." This is a clean, well-documented AI-hallucination → legal liability → direct cash outflow chain. ≈ 700 USD (C$650.88 fare difference + C$36.14 interest + C$125 fees ≈ US$721). |
| Int2 | https://www.techradar.com/pro/deloitte-forced-to-refund-aussie-government-after-admitting-it-used-ai-to-produce-error-strewn-report | 2024-09 (estimate) | Deloitte, GPT-4o, Australia's Department of Employment and Workplace Relations | Deloitte used GPT-4o to draft a report for Australia's Department of Employment and Workplace Relations; the report contained fake citations, non-existent quotations and other hallucinations, forcing the government to correct and re-upload it and Deloitte to repay the final installment of a AU$440k contract. | Highly salient example of an LLM-generated expert-style document in a regulated public-policy context. The consulting firm had to refund the state and suffered reputational damage; the ministry had to spend internal resources to repair the report. Shows how hallucinated legal/administrative content can directly affect procurement and public-sector governance. Exact refund unknown; upper bound ≈ 290,000 USD (total contract AU$440k). Real refund was the "final instalment," almost certainly a low six-figure amount. |
| Int3 | https://www.goldbergsegalla.com/news-and-knowledge/insights/fake-cases-real-consequences-two-recent-opinions-demonstrate-the-continuing-risks-of-generative-ai-in-litigation | 2023-06 (estimate) | ChatGPT, Mata v. Avianca lawyers, S.D.N.Y. court | Mata v. Avianca (S.D.N.Y.): lawyers submitted a brief with multiple non-existent cases generated by ChatGPT; the court sanctioned them US$5,000 and required notification of all affected judges. | Legal domain but strongly relevant to finance/life-science compliance: when legal teams in regulated industries lean on LLM research, hallucinated citations can directly trigger sanctions and weaken the client's position. ≥ 5,000 USD in direct sanctions, plus unquantified extra defense costs, internal reviews, and reputational damage. |
| Int4 | https://www.washingtonpost.com/nation/2025/06/03/attorneys-court-ai-hallucinations-judges | 2025-06 | Damien Charlotin's AI Hallucination Cases Database, U.S. courts, lawyers/firms | Washington Post analysis using Damien Charlotin's AI Hallucination Cases Database: at least 95 U.S. court matters since mid-2023 involve AI-hallucinated citations; in several, courts imposed fines of "thousands of dollars" on lawyers and firms. | Charlotin's database is now a de facto registry of AI-tainted filings. In heavily regulated sectors (banking, pharma, med-tech) where litigation risk is structurally high, any legal team using LLMs without controls is now demonstrably exposed to monetary sanctions and malpractice / D&O risk. At least tens of thousands of USD in aggregate sanctions; exact total not yet systematically tallied. |
